{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score,recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Helper Functions \n",
    "# How to encode Categorical variables: https://data-newbie.tistory.com/90\n",
    "def ordinal_encoding(df):\n",
    "    enc = OrdinalEncoder()\n",
    "    return enc.fit_transform(df[cat_col].values)\n",
    "def onehot_encoding(df):\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    return enc.fit_transform(df[cat_col].values)\n",
    "def _eval(model, X, y):    \n",
    "    pred_y = model.predict(X)\n",
    "    prob = model.predict_proba(X)\n",
    "    true_y = y\n",
    "    acc = accuracy_score(y_pred=pred_y,y_true=true_y)\n",
    "#     auroc = roc_auc_score(y_true=y, y_score=prob,multi_class='ovr') # standard ovr auroc\n",
    "    f1 = f1_score(y_pred=pred_y,y_true=true_y,average='macro')\n",
    "    precision, recall = precision_score(y_pred=pred_y,y_true=true_y,average='macro'),recall_score(y_pred=pred_y,y_true=true_y,average='macro')\n",
    "    #,auroc\n",
    "    return acc,f1,precision,recall\n",
    "## SETTINGS and VARIABLES \n",
    "\n",
    "enc_method = 'onehot'\n",
    "# Split categorical variables and numerical variables\n",
    "cat_col = ['AgencyCode','RecipientCode','RegionCode']\n",
    "num_col = ['randomNumCol1','randomNumCol2']\n",
    "# Basic classifiers\n",
    "model_names = ['SVM','RF','MLP','NB'] # SVM , Random Forest,' Multi Layer Perceptron' ,'Naive Bayes'\n",
    "model_name='RF'\n",
    "# LOAD DATASET\n",
    "data = pd.read_csv(\"data/crs_final_df_kor.csv\", header = 0)\n",
    "# add \n",
    "data[['AgencyCode','RecipientCode','RegionCode','IncomegroupCode','FlowCode','Bi_Multi']]\n",
    "\n",
    "### add random numerical column (since I dont have more numerical columns in this data)\n",
    "data['randomNumCol1'] = [np.random.randint(1,10) for i in range(data.shape[0])]\n",
    "data['randomNumCol2'] = [np.random.rand() for i in range(data.shape[0])]\n",
    "### \n",
    "\n",
    "# Split categorical variables and numerical variables\n",
    "cat_col = ['AgencyCode','RecipientCode','RegionCode']\n",
    "num_col = ['randomNumCol1','randomNumCol2']\n",
    "\n",
    "if enc_method =='ordinal':\n",
    "    x_cat = ordinal_encoding(data)\n",
    "elif enc_method=='onehot':\n",
    "    x_cat = onehot_encoding(data)\n",
    "else:\n",
    "    raise\n",
    "\n",
    "x_num = data[num_col].values\n",
    "X = np.hstack([x_cat.toarray(),x_num])\n",
    "# print(x_cat.shape,x_num.shape,X.shape)\n",
    "y = data['PurposeCode'].values\n",
    "y_set = list(set(y))\n",
    "y = [y_set.index(i) for i in y]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "# MAIN\n",
    "if model_name=='SVM':\n",
    "    model = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)) # SVM\n",
    "elif model_name=='RF':\n",
    "    model = RandomForestClassifier(n_estimators=5,random_state=42) # RF\n",
    "elif model_name=='MLP':\n",
    "    model = MLPClassifier(random_state=42, max_iter=300) # MLP\n",
    "elif model_name == 'NB':\n",
    "    model = GaussianNB() #NB \n",
    "elif model_name=='XGB':\n",
    "    model = XGBClassifier(eval_metric='mlogloss')\n",
    "model.fit(X_train,y_train)\n",
    "acc,f1,prec,recall = _eval(model,X_train,y_train)\n",
    "print('train:',model_name,acc,f1,prec,recall)\n",
    "acc,f1,prec,recall = _eval(model,X_test,y_test)\n",
    "print('test:',model_name,acc,f1,prec,recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70269, 183), (56215, 183))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "\n",
    "data = load_data()\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4\"\n",
    "# available_gpu = [0,1,2,3,4]\n",
    "data['PurposeCode'] = data['PurposeCode'].apply(lambda x: int(float(x)))\n",
    "n_cls = list(data['PurposeCode'].unique())\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "train_corpus, train_targets, test_corpus, test_targets ,val_corpus, val_targets = train_test_split (data,0.75,0.25,0.0,n_cls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,  bert,num_cls):\n",
    "        super(Model, self).__init__()\n",
    "        self.dim = 768\n",
    "        self.encoder = BertModel.from_pretrained(bert)\n",
    "        # self.fc = nn.Linear(self.dim, num_cls)\n",
    "        self.hidden = 100\n",
    "        self.mlp_projection =  nn.Sequential(nn.Linear(self.dim,self.hidden),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(self.hidden,self.hidden,bias=True))\n",
    "        self.mlp_prediction =  nn.Sequential(nn.Linear(self.dim,self.hidden),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(self.hidden,num_cls,bias=True))\n",
    "        #nn.Linear(self.dim,self.hidden), nn.ReLU(),nn.Linear(self.hidden,num_cls)\n",
    "    def forward(self, input_ids, attention_mask,ce=False):\n",
    "        output = self.encoder(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        embedding = output['pooler_output']\n",
    "        if ce:\n",
    "            return self.mlp_prediction(embedding)\n",
    "        else:\n",
    "            return self.mlp_projection(embedding)\n",
    "class CRSdataset(Dataset):\n",
    "    def __init__(self, model_name, targets, text_list, max_len = 512):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.data = []\n",
    "        self.max_len=max_len\n",
    "        self.targets = targets\n",
    "        for text in tqdm(text_list):\n",
    "            org_input = self.tokenizer(text, padding='max_length', truncation=True,\n",
    "                                       max_length=self.max_len, return_tensors='pt')\n",
    "            org_input['input_ids'] = torch.squeeze(org_input['input_ids'])\n",
    "            org_input['attention_mask'] = torch.squeeze(org_input['attention_mask'])\n",
    "            self.data.append(org_input)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx],self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b4068b4f9d4935904953bf2cf3e6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab158aa348f479a873b087f6ccb1908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15128]) torch.Size([15128])\n"
     ]
    }
   ],
   "source": [
    "bsz = 8\n",
    "n_class = len(data['PurposeCode'].unique())\n",
    "model_name = 'bert-base-uncased'\n",
    "model = Model(bert=model_name, num_cls = n_class)\n",
    "available_gpu = [0,3]\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model,device_ids=available_gpu) # device_ids=device_ids\n",
    "# model.cpu()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('../bert_0816.pth'),strict=True)\n",
    "testds = CRSdataset(model_name=model_name,targets=test_targets,text_list=test_corpus)\n",
    "testloader = DataLoader(testds, batch_size=bsz, shuffle=False, num_workers=1,drop_last=True)\n",
    "model.eval()\n",
    "import sklearn.metrics\n",
    "with torch.no_grad():\n",
    "    tbar= tqdm(testloader)\n",
    "    all_outputs = []\n",
    "    test_y = []\n",
    "    loss = []\n",
    "    for inputs, targets in tbar:\n",
    "        input_ids = inputs['input_ids'].long().cuda()\n",
    "        attention_mask = inputs['attention_mask'].long().cuda()\n",
    "        output = model(input_ids=input_ids,attention_mask=attention_mask,ce=True)\n",
    "        all_outputs.append(output.cpu())\n",
    "        test_y.append(targets)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_losses = torch.mean(torch.tensor(loss))\n",
    "    test_y = torch.cat(test_y)\n",
    "    val_preds = all_outputs.softmax(dim=1)\n",
    "    pred_y = val_preds.argmax(axis=1) \n",
    "    print(pred_y.shape,test_y.shape)\n",
    "    # val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "    acc = sklearn.metrics.accuracy_score(y_pred=pred_y,y_true=test_y)\n",
    "    f1 = sklearn.metrics.f1_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "    auc = sklearn.metrics.roc_auc_score(y_true=test_y,y_score=val_preds,multi_class='ovr')\n",
    "    prec = sklearn.metrics.precision_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "    recall = sklearn.metrics.recall_score(y_true=test_y,y_pred=pred_y,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert: 0.8972104706504495 0.8782385829877718 0.8875509801412697 0.8765621279373174\n"
     ]
    }
   ],
   "source": [
    "print('bert:',acc,f1,prec,recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electra model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c8bd2ab93b4227ac6cfb2729551825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7144ba004204a6789607cc3f43184b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15128]) torch.Size([15128])\n",
      "electra: 0.8972765732416711 0.8740787824992978 0.8856782858958924 0.872673920728798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bsz = 8\n",
    "n_class = len(data['PurposeCode'].unique())\n",
    "model_name = 'google/electra-small-discriminator'\n",
    "model = Model_electra_wofreeze(model_name=model_name, num_cls = n_class)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model,device_ids=available_gpu) # device_ids=device_ids\n",
    "# model.cpu()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('electra_model_0816.pth'),strict=True)\n",
    "testds = CRSdataset(model_name=model_name,targets=test_targets,text_list=test_corpus)\n",
    "testloader = DataLoader(testds, batch_size=bsz, shuffle=False, num_workers=1,drop_last=True)\n",
    "model.eval()\n",
    "import sklearn.metrics\n",
    "with torch.no_grad():\n",
    "    tbar= tqdm(testloader)\n",
    "    all_outputs = []\n",
    "    test_y = []\n",
    "    loss = []\n",
    "    for inputs, targets in tbar:\n",
    "        input_ids = inputs['input_ids'].long().cuda()\n",
    "        attention_mask = inputs['attention_mask'].long().cuda()\n",
    "        output = model(input_ids=input_ids,attention_mask=attention_mask,ce=True)\n",
    "        all_outputs.append(output.cpu())\n",
    "        test_y.append(targets)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "    all_losses = torch.mean(torch.tensor(loss))\n",
    "    test_y = torch.cat(test_y)\n",
    "    val_preds = all_outputs.softmax(dim=1)\n",
    "    pred_y = val_preds.argmax(axis=1) \n",
    "    print(pred_y.shape,test_y.shape)\n",
    "    # val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "    acc = sklearn.metrics.accuracy_score(y_pred=pred_y,y_true=test_y)\n",
    "    f1 = sklearn.metrics.f1_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "    auc = sklearn.metrics.roc_auc_score(y_true=test_y,y_score=val_preds,multi_class='ovr')\n",
    "    prec = sklearn.metrics.precision_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "    recall = sklearn.metrics.recall_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "print('electra:',acc,f1,prec,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import top_k_accuracy_score\n",
    "import torch\n",
    "from torch import tensor\n",
    "from sklearn.metrics import f1_score, accuracy_score, jaccard_score,precision_score,recall_score\n",
    "# sourcecode from  https://gist.github.com/weiaicunzai/2a5ae6eac6712c70bde0630f3e76b77b\n",
    "def top_k_eval(logits, y, k : int = 1):\n",
    "    \"\"\"\n",
    "    logits : (bs, n_labels)\n",
    "    y : (bs,)\n",
    "    \"\"\"\n",
    "    labels_dim = 1\n",
    "    assert 1 <= k <= logits.size(labels_dim)\n",
    "    k_labels = torch.topk(input = logits, k = k, dim=labels_dim, largest=True, sorted=True)[1]\n",
    "\n",
    "    # True (#0) if `expected label` in k_labels, False (0) if not\n",
    "    a = ~torch.prod(input = torch.abs(y.unsqueeze(labels_dim) - k_labels), dim=labels_dim).to(torch.bool)\n",
    "    \n",
    "    # These two approaches are equivalent\n",
    "    if False :\n",
    "        y_pred = torch.empty_like(y)\n",
    "        for i in range(y.size(0)):\n",
    "            if a[i] :\n",
    "                y_pred[i] = y[i]\n",
    "            else :\n",
    "                y_pred[i] = k_labels[i][0]\n",
    "        #correct = a.to(torch.int8).numpy()\n",
    "    else :\n",
    "        a = a.to(torch.int8)\n",
    "        y_pred = a * y + (1-a) * k_labels[:,0]\n",
    "        #correct = a.numpy()\n",
    "\n",
    "#     f1 = f1_score(y_pred, y, average='weighted')*100\n",
    "    #acc = sum(correct)/len(correct)*100\n",
    "    acc = accuracy_score(y_pred, y)#*100\n",
    "    f1 = sklearn.metrics.f1_score(y_true=y,y_pred=y_pred,average='macro')\n",
    "    prec = sklearn.metrics.precision_score(y_true=y,y_pred=y_pred,average='macro')\n",
    "    recall = sklearn.metrics.recall_score(y_true=y,y_pred=y_pred,average='macro')\n",
    "    return acc, f1,prec,recall\n",
    "\n",
    "topk_results = []\n",
    "for k in [1,3,5]:\n",
    "    topk_acc,topk_f1,topk_prec,topk_recall = top_k_eval(logits=val_preds,y=test_y,k=k)\n",
    "#     print(topk_acc,topk_acc2,topk_prec,topk_recall,topk_f1)\n",
    "    topk_results.append([topk_acc,topk_f1,topk_prec,topk_recall,k])\n",
    "topk_results = pd.DataFrame(topk_results,columns=['acc','f1','prec','recall','k']).set_index('k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>f1</th>\n",
       "      <th>prec</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.897277</td>\n",
       "      <td>0.874079</td>\n",
       "      <td>0.885678</td>\n",
       "      <td>0.872674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.957496</td>\n",
       "      <td>0.944474</td>\n",
       "      <td>0.950573</td>\n",
       "      <td>0.940359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970518</td>\n",
       "      <td>0.957421</td>\n",
       "      <td>0.962017</td>\n",
       "      <td>0.954112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc        f1      prec    recall\n",
       "k                                        \n",
       "1  0.897277  0.874079  0.885678  0.872674\n",
       "3  0.957496  0.944474  0.950573  0.940359\n",
       "5  0.970518  0.957421  0.962017  0.954112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM, CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader_cnn_bilstm import *\n",
    "batch_size = 512\n",
    "valid_loader, test_y, x_cv, le =  load_data_cnn_bilstm(data_path = \"data/crs_final_df_kor.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8724810042946812 0.8445913147821237 0.9964722572607925 0.8784275814401158 0.8253003129921374\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('results/textcnn_model')\n",
    "model.eval()        \n",
    "avg_val_loss = 0.\n",
    "val_preds = np.zeros((len(x_cv),len(le.classes_))) \n",
    "# keep/store predictions\n",
    "with torch.no_grad(): # You should use no_grad session when you evaluate the model. (It is much faster!)\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "        y_pred = model(x_batch).detach()\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "# Check Accuracy\n",
    "pred_y = val_preds.argmax(axis=1) \n",
    "import sklearn.metrics\n",
    "# val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "acc = sklearn.metrics.accuracy_score(y_pred=pred_y,y_true=test_y)\n",
    "f1 = sklearn.metrics.f1_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "auc = sklearn.metrics.roc_auc_score(y_true=test_y,y_score=val_preds,multi_class='ovr')\n",
    "prec = sklearn.metrics.precision_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "recall = sklearn.metrics.recall_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "print(acc,f1,auc,prec,recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('results/bilstm_model')\n",
    "model.eval()        \n",
    "avg_val_loss = 0.\n",
    "val_preds = np.zeros((len(x_cv),len(le.classes_))) \n",
    "\n",
    "# keep/store predictions\n",
    "with torch.no_grad(): # You should use no_grad session when you evaluate the model. (It is much faster!)\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader): \n",
    "        y_pred = model(x_batch).detach()\n",
    "        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "# Check Accuracy\n",
    "pred_y = val_preds.argmax(axis=1) \n",
    "import sklearn.metrics\n",
    "# val_accuracy = sum(val_preds.argmax(axis=1)==test_y)/len(test_y)\n",
    "acc = sklearn.metrics.accuracy_score(y_pred=pred_y,y_true=test_y)\n",
    "f1 = sklearn.metrics.f1_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "auc = sklearn.metrics.roc_auc_score(y_true=test_y,y_score=val_preds,multi_class='ovr')\n",
    "prec = sklearn.metrics.precision_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "recall = sklearn.metrics.recall_score(y_true=test_y,y_pred=pred_y,average='macro')\n",
    "print(acc,f1,auc,prec,recall)\n",
    "#0.8197555335315494 0.7206335487417044 0.9901975804315533 0.7829225534517649 0.6965562367234499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simsimi",
   "language": "python",
   "name": "simsimi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
